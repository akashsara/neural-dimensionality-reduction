{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naked-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desirable-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stopped-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-union",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "japanese-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.05\n",
    "validation_split = 0.05\n",
    "learning_rate = 1e-3\n",
    "input_size = 300\n",
    "hidden_size = 225\n",
    "bottleneck_size = 200\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "use_hidden_layer = False\n",
    "denoising = 'masking'\n",
    "noise_factor = 0.3\n",
    "alpha_weight = 1\n",
    "beta_weight = 0.5\n",
    "experiment_name = f\"autoencoder_300to{bottleneck_size}_v3.1\"\n",
    "dataset = \"data/embeddings/base/clipped.glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "killing-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "clinical-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False cpu\n"
     ]
    }
   ],
   "source": [
    "print(gpu, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-browser",
   "metadata": {},
   "source": [
    "# Load & Prepare Embeddings for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surprising-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "vectors = []\n",
    "with open(dataset, \"r\", encoding='utf8') as fp:\n",
    "    for line in fp:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        vector = np.asarray(line[1:], 'float32')\n",
    "        words.append(word)\n",
    "        vectors.append(vector)\n",
    "vectors = torch.from_numpy(np.asarray(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "current-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = int(test_split * len(words))\n",
    "validation_split = int(validation_split* len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "civilian-motion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 torch.Size([400000, 300])\n",
      "20000 20000\n"
     ]
    }
   ],
   "source": [
    "print(len(words), vectors.shape)\n",
    "print(test_split, validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "patient-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_words, test_words = train_test_split(\n",
    "    vectors, words, test_size=test_split, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "other-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, validation_vectors, train_words, validation_words = train_test_split(\n",
    "    train_vectors, train_words, test_size=validation_split, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "falling-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([360000, 300]), torch.Size([20000, 300]), torch.Size([20000, 300]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.shape, validation_vectors.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specified-gentleman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360000, 20000, 20000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: We don't actually use these words since the model doesn't care about them.\n",
    "# We just compute them in case we want to check some particular word or something.\n",
    "len(train_words), len(validation_words), len(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "animal-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_vectors, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=gpu\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_vectors, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=gpu\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_vectors, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-peeing",
   "metadata": {},
   "source": [
    "# Model Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "latin-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderWithoutHiddenLayer(nn.Module):\n",
    "    def __init__(self, input_size, bottleneck_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(in_features=input_size, out_features=bottleneck_size)\n",
    "        self.encoder_activation = nn.Tanh()\n",
    "        \n",
    "        self.decoder = nn.Linear(in_features=bottleneck_size, out_features=input_size)\n",
    "        self.decoder_activation = nn.Tanh()\n",
    "        \n",
    "        self.decoder.weight = nn.Parameter(self.encoder.weight.transpose(0,1))\n",
    "    \n",
    "    def forward(self, features):\n",
    "        latent_representation = self.encoder_activation(self.encoder(features))\n",
    "        reconstructed_input = self.decoder_activation(self.decoder(latent_representation))\n",
    "        return reconstructed_input\n",
    "    \n",
    "    def encode(self, features):\n",
    "        return self.encoder_activation(self.encoder(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "small-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderWithHiddenLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bottleneck_size):\n",
    "        super().__init__()\n",
    "        self.encoder_input = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        self.encoder_input_activation = nn.ReLU(True)\n",
    "        self.encoder_hidden = nn.Linear(in_features=hidden_size, out_features=bottleneck_size)\n",
    "        self.encoder_hidden_activation = nn.Tanh()\n",
    "        \n",
    "        self.decoder_hidden = nn.Linear(in_features=bottleneck_size, out_features=hidden_size)\n",
    "        self.decoder_hidden_activation = nn.ReLU(True)\n",
    "        self.decoder_output = nn.Linear(in_features=hidden_size, out_features=input_size)\n",
    "        self.decoder_output_activation = nn.Tanh()\n",
    "        \n",
    "        self.decoder_hidden.weight = nn.Parameter(self.encoder_hidden.weight.transpose(0,1))\n",
    "        self.decoder_output.weight = nn.Parameter(self.encoder_input.weight.transpose(0,1))\n",
    "        \n",
    "        self.encoder = [\n",
    "            self.encoder_input, self.encoder_input_activation, self.encoder_hidden, self.encoder_hidden_activation\n",
    "        ]\n",
    "        self.decoder = [\n",
    "            self.decoder_hidden, self.decoder_hidden_activation, self.decoder_output, self.decoder_output_activation\n",
    "        ]\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Encoder\n",
    "        for layer in self.encoder:\n",
    "            features = layer(features)\n",
    "        # Decoder\n",
    "        for layer in self.decoder:\n",
    "            features = layer(features)\n",
    "        return features\n",
    "    \n",
    "    def encode(self, features):\n",
    "        for layer in self.encoder:\n",
    "            features = layer(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "quarterly-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_hidden_layer:\n",
    "    model = AutoEncoderWithHiddenLayer(input_size, hidden_size, bottleneck_size).to(device)\n",
    "else:\n",
    "    model = AutoEncoderWithoutHiddenLayer(input_size, bottleneck_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "referenced-london",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoderWithoutHiddenLayer(\n",
       "  (encoder): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (encoder_activation): Tanh()\n",
       "  (decoder): Linear(in_features=200, out_features=300, bias=True)\n",
       "  (decoder_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "disabled-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masking_noise(shape, amount):\n",
    "    \"\"\"\n",
    "    Shape: Shape of the noise matrix\n",
    "    Amount: The amount of the matrix that should be masked (zero'd) out\n",
    "    \"\"\"\n",
    "    mask = np.ones(shape)\n",
    "    amount = int(shape[0] * amount)\n",
    "    mask[:, :amount] = 0\n",
    "    for x in mask:\n",
    "        np.random.shuffle(x)\n",
    "    return torch.from_numpy(mask.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "individual-xerox",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2813/2813 [00:18<00:00, 153.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 145.76it/s]\n",
      "  0%|                                                                                         | 0/2813 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Train Reconstruction Loss = 0.02092375290516196,         Validation Reconstruction Loss = 0.017712685368528033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2813/2813 [00:19<00:00, 144.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 127.10it/s]\n",
      "  0%|                                                                                         | 0/2813 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5, Train Reconstruction Loss = 0.020048815937142674,         Validation Reconstruction Loss = 0.017682940206804852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2813/2813 [00:18<00:00, 150.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 124.59it/s]\n",
      "  0%|                                                                                         | 0/2813 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5, Train Reconstruction Loss = 0.02001157213218306,         Validation Reconstruction Loss = 0.01759489730095408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2813/2813 [00:20<00:00, 140.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 138.78it/s]\n",
      "  0%|                                                                                         | 0/2813 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5, Train Reconstruction Loss = 0.019991760944349672,         Validation Reconstruction Loss = 0.017615610164157143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2813/2813 [00:20<00:00, 140.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 127.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5, Train Reconstruction Loss = 0.019973666228054684,         Validation Reconstruction Loss = 0.017561830864970093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    validation_loss = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # Reset gradients back to zero for this iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if denoising == 'additive':\n",
    "            # Add noise to our inputs\n",
    "            noise = torch.randn(batch.shape) * noise_factor\n",
    "            noisy_batch = torch.clamp(batch + noise, -1, +1)\n",
    "            \n",
    "            # Move batch to device\n",
    "            noisy_batch = noisy_batch.to(device)\n",
    "\n",
    "            # Run our model & get outputs\n",
    "            outputs = model(noisy_batch)\n",
    "            \n",
    "            # Calculate reconstruction loss\n",
    "            batch_loss = criterion(outputs, batch)\n",
    "        elif denoising == 'masking':\n",
    "            # Create masking noise and mask inputs\n",
    "            noise = create_masking_noise(batch.shape, noise_factor)\n",
    "            masked_batch = np.multiply(batch, noise)\n",
    "            \n",
    "            # Move batch to device\n",
    "            masked_batch = masked_batch.to(device)\n",
    "            \n",
    "            # Run model & get outputs\n",
    "            outputs = model(masked_batch)\n",
    "            \n",
    "            # Calculate reconstruction loss\n",
    "            # We calculate the error for the masked dimensions separately from the unmasked ones\n",
    "            # We can then assign a weight to each of the two components \n",
    "            unmasked_error = criterion(outputs * noise, masked_batch)\n",
    "            masked_error = criterion(outputs * (1 - noise), batch * (1 - noise))\n",
    "            batch_loss = (alpha_weight * masked_error) + (beta_weight * unmasked_error)\n",
    "        else:\n",
    "            # Move batch to device\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Run our model & get outputs\n",
    "            outputs = model(batch)\n",
    "            \n",
    "            # Calculate reconstruction loss\n",
    "            batch_loss = criterion(outputs, batch)\n",
    "                  \n",
    "        # Backprop\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # Update our optimizer parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add the batch's loss to the total loss for the epoch\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "    # Validation Loop\n",
    "    with torch.no_grad():\n",
    "        for iteration, batch in enumerate(tqdm(validation_dataloader)):\n",
    "            # Move batch to device\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Run our model & get outputs\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # Calculate reconstruction loss\n",
    "            batch_loss = criterion(outputs, batch)\n",
    "\n",
    "            # Add the batch's loss to the total loss for the epoch\n",
    "            validation_loss += batch_loss.item()\n",
    "    \n",
    "    # Compute the average losses for this epoch\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    validation_loss = validation_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Print Metrics\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}/{epochs}, Train Reconstruction Loss = {train_loss}, \\\n",
    "        Validation Reconstruction Loss = {validation_loss}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-kingston",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "flexible-roots",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoderWithoutHiddenLayer(\n",
       "  (encoder): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (encoder_activation): Tanh()\n",
       "  (decoder): Linear(in_features=200, out_features=300, bias=True)\n",
       "  (decoder_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "governing-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 135.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Reconstruction Loss = 0.01756591818467447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss = 0\n",
    "\n",
    "# Testing Loop\n",
    "with torch.no_grad():\n",
    "    for iteration, batch in enumerate(tqdm(test_dataloader)):\n",
    "        # Move batch to device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Run our model & get outputs\n",
    "        outputs = model(batch)\n",
    "\n",
    "        # Calculate reconstruction loss\n",
    "        batch_loss = criterion(outputs, batch)\n",
    "\n",
    "        # Add the batch's loss to the total loss for the epoch\n",
    "        reconstruction_loss += batch_loss.item()\n",
    "\n",
    "# Compute the average losses for this epoch\n",
    "reconstruction_loss = reconstruction_loss / len(test_dataloader)\n",
    "\n",
    "# Print Metrics\n",
    "print(\n",
    "    f\"Test Reconstruction Loss = {reconstruction_loss}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-stanley",
   "metadata": {},
   "source": [
    "# Generate Latent Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "sophisticated-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:20, 19450.47it/s]\n"
     ]
    }
   ],
   "source": [
    "latent_vectors = {}\n",
    "with torch.no_grad():\n",
    "    for i, (word, vector) in enumerate(tqdm(zip(words, vectors))):\n",
    "        latent_vectors[word] = model.encode(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "inside-handling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "suited-projector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0491, -0.7057,  0.5585,  0.4505,  0.1197,  0.8171,  0.2767,  0.1517,\n",
       "         0.2609, -0.3418, -0.5732, -0.2670, -0.5859, -0.6228, -0.8439,  0.5550,\n",
       "         0.4134, -0.2099,  0.0208,  0.4179, -0.0282, -0.3089,  0.1710, -0.6101,\n",
       "        -0.1411, -0.1068,  0.1119,  0.4446, -0.0538, -0.1984,  0.4466, -0.0773,\n",
       "        -0.1837, -0.0393,  0.0667,  0.5724, -0.0938, -0.2321, -0.1937, -0.0537,\n",
       "         0.4824, -0.6321,  0.6465,  0.0949, -0.6390, -0.2557, -0.7237,  0.3523,\n",
       "         0.5180, -0.2043, -0.2086, -0.5812,  0.3521, -0.4286,  0.0879, -0.7394,\n",
       "        -0.6898, -0.2148,  0.3370,  0.0734, -0.7275,  0.2699,  0.2641,  0.3486,\n",
       "        -0.1092, -0.4515, -0.3972, -0.3416,  0.4906,  0.1302, -0.1905, -0.1336,\n",
       "         0.7853, -0.0809,  0.5394, -0.2317,  0.0281, -0.1255,  0.4968,  0.4325,\n",
       "        -0.3001, -0.0125,  0.0547,  0.0385,  0.4382, -0.2669, -0.6018, -0.1798,\n",
       "         0.4208,  0.0494, -0.0710, -0.0635, -0.5309,  0.6894,  0.0994, -0.7257,\n",
       "        -0.3527, -0.1521, -0.5692, -0.0652,  0.4916, -0.2610, -0.4154,  0.3411,\n",
       "        -0.3745, -0.3321, -0.5186, -0.4271,  0.5190, -0.4675, -0.4966, -0.7670,\n",
       "        -0.0895, -0.0646,  0.0980, -0.1937,  0.4361, -0.7046,  0.4977,  0.6470,\n",
       "        -0.1182, -0.4535,  0.0017,  0.1028, -0.0998, -0.4577,  0.0047, -0.7099,\n",
       "        -0.1192, -0.2856,  0.4821,  0.3420,  0.0240,  0.5125, -0.0193,  0.6109,\n",
       "        -0.3194,  0.1371, -0.3653, -0.0269,  0.3270, -0.0843, -0.6345,  0.3685,\n",
       "         0.5707, -0.1648, -0.2348, -0.6009,  0.2551, -0.4675,  0.2476, -0.4661,\n",
       "         0.6640, -0.1404,  0.1434,  0.4219, -0.4579,  0.4158,  0.1895,  0.6423,\n",
       "         0.6205,  0.0496,  0.2186, -0.1447,  0.1240, -0.4457, -0.6405, -0.3504,\n",
       "        -0.7947, -0.3799, -0.1683, -0.1800, -0.4886, -0.4044,  0.6078,  0.2259,\n",
       "         0.1438, -0.0637, -0.4361, -0.4075, -0.2309,  0.5155,  0.1308, -0.2155,\n",
       "        -0.4508,  0.4869,  0.1836,  0.7534, -0.2408, -0.0230,  0.5945, -0.2148,\n",
       "        -0.5348, -0.4192, -0.0832, -0.7728, -0.2074,  0.5216,  0.2958, -0.2282])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vectors['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-mount",
   "metadata": {},
   "source": [
    "# Save Model & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "political-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"models/{experiment_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "blond-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:57, 6906.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Need to convert the latent embeddings into the glove format\n",
    "# word dim1 dim2 dim3 dim4 ... dimX\n",
    "lines = []\n",
    "for i, (word, vector) in tqdm(enumerate(latent_vectors.items())):\n",
    "    line = [word] + [str(x) for x in vector.tolist()]\n",
    "    lines.append(' '.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "wooden-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/embeddings/trained/{experiment_name}.glove.6B.300d.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-desire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
