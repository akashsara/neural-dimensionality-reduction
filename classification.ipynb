{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-artist",
   "metadata": {
    "id": "piano-artist"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-designation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "advisory-designation",
    "outputId": "73c25ad3-d009-4274-8761-d5313a51ee67"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sDy2wkshvgGi",
   "metadata": {
    "id": "sDy2wkshvgGi"
   },
   "source": [
    "Colab-Specific Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQZjfi86vZLg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQZjfi86vZLg",
    "outputId": "14ca38e9-83b7-42e1-cf80-703b6d4132f4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-mills",
   "metadata": {
    "id": "alert-mills"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-foundation",
   "metadata": {
    "id": "molecular-foundation"
   },
   "outputs": [],
   "source": [
    "colab_prefix = \"drive/MyDrive/CMPUT651_DL4NLP/\"\n",
    "\n",
    "source_embedding_dim = 300\n",
    "version = 1\n",
    "model_type = \"supervised_distilled\"\n",
    "embedding_dim = 100\n",
    "nn_hidden_size = 50\n",
    "\n",
    "experiment_name = f\"{model_type}_{source_embedding_dim}to{embedding_dim}_v{version}\"\n",
    "embedding_path = colab_prefix + f\"data/embeddings/trained/{experiment_name}.glove.6B.300d.txt\"\n",
    "model_output_path = colab_prefix + f\"models/classifier_glove_{experiment_name}.pt\"\n",
    "# embedding_path = colab_prefix + f\"data/embeddings/base/glove.6B.300d.txt\"\n",
    "# model_output_path = colab_prefix + f\"models/classifier_hidden10_glove_clipped_300d.pt\"\n",
    "\n",
    "freeze_embeddings = True\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_datapath = colab_prefix + \"data/datasets/ag_news/train.csv\"\n",
    "test_datapath = colab_prefix + \"data/datasets/ag_news/test.csv\"\n",
    "\n",
    "pad_tag = \"<PAD>\"\n",
    "unk_tag = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-alabama",
   "metadata": {
    "id": "institutional-alabama"
   },
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62OAFRIK3Tfo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62OAFRIK3Tfo",
    "outputId": "fd39c74a-94e7-49e2-894d-0121ec269e5a"
   },
   "outputs": [],
   "source": [
    "print(experiment_name)\n",
    "print(gpu, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-chinese",
   "metadata": {
    "id": "worst-chinese"
   },
   "source": [
    "# Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-living",
   "metadata": {
    "id": "amazing-living"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "vectors = []\n",
    "with open(embedding_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    for line in fp:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        vector = np.asarray(line[1:], dtype='float32')\n",
    "        words.append(word)\n",
    "        vectors.append(vector)\n",
    "vectors = np.asarray(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-appraisal",
   "metadata": {
    "id": "charged-appraisal"
   },
   "source": [
    "Create an embedding for both \\<PAD> (all 0s) and \\<UNK> (average of all embeddings) tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-atlanta",
   "metadata": {
    "id": "strong-atlanta"
   },
   "outputs": [],
   "source": [
    "unk_embedding = np.mean(vectors, axis=0).reshape(1, -1)\n",
    "pad_embedding = np.zeros((1, vectors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-leather",
   "metadata": {
    "id": "secondary-leather"
   },
   "outputs": [],
   "source": [
    "vectors = torch.as_tensor(np.concatenate((vectors, pad_embedding, unk_embedding)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-mongolia",
   "metadata": {
    "id": "surgical-mongolia"
   },
   "source": [
    "Set up dictionaries for converting tags to indices, tokens to indices and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-universal",
   "metadata": {
    "id": "compliant-universal"
   },
   "outputs": [],
   "source": [
    "token2index = {word: i for i, word in enumerate(words)}\n",
    "pad_token_index = len(token2index)\n",
    "unk_token_index = len(token2index) + 1\n",
    "token2index[pad_tag] = pad_token_index\n",
    "token2index[unk_tag] = unk_token_index\n",
    "\n",
    "index2token = {i: word for word, i in token2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag ={\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-marijuana",
   "metadata": {
    "id": "intelligent-marijuana"
   },
   "source": [
    "# Load Data & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, token2index, unk_token_index):\n",
    "    return [token2index.get(word, unk_token_index) for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_sentence(sentence, index2token):\n",
    "    return [index2token[int(index)] for index in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max_length(sentence, pad_token_index, max_length):\n",
    "    padding = [pad_token_index] * (max_length - len(sentence))\n",
    "    return sentence + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, token2index, pad_token_index, unk_token_index, title_max_length=0, desc_max_length=0):\n",
    "    print(\"Splitting & Lowercasing.\")\n",
    "    df['Title'] = df['Title'].str.lower().str.split()\n",
    "    df['Description'] = df['Description'].str.lower().str.split()\n",
    "    \n",
    "    # If no max length is specified, compute from data\n",
    "    if (title_max_length == desc_max_length) and (title_max_length == 0):\n",
    "        print(\"Computing Max Lengths.\")\n",
    "        title_max_length = df['Title'].apply(len).max()\n",
    "        desc_max_length = df['Description'].apply(len).max()\n",
    "    \n",
    "    # Convert tokens to indices\n",
    "    print(\"Transforming tokens into indices.\")\n",
    "    df['Title'] = df['Title'].apply(sentence_to_indices, args=(token2index, unk_token_index))\n",
    "    df['Description'] = df['Description'].apply(sentence_to_indices, args=(token2index, unk_token_index))\n",
    "    \n",
    "    # Pad data\n",
    "    print(\"Padding data.\")\n",
    "    df['Title'] = df['Title'].apply(pad_to_max_length, args=(pad_token_index, title_max_length))\n",
    "    df['Description'] = df['Description'].apply(pad_to_max_length, args=(pad_token_index, desc_max_length))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    print(\"Splitting features & labels and converting to tensors.\")\n",
    "    data = df.to_dict(orient=\"records\")\n",
    "    titles = [x['Title'] for x in data]\n",
    "    descriptions = [x['Description'] for x in data]\n",
    "    features = [torch.as_tensor(titles), torch.as_tensor(descriptions)]\n",
    "    labels = torch.as_tensor([x['Class Index'] for x in data]) - 1 # We want 0-3 not 1-4\n",
    "    return features, labels, title_max_length, desc_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_datapath)\n",
    "test = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(\n",
    "    train, test_size=4000, stratify=train['Class Index'], random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, title_max_length, desc_max_length = preprocess_data(\n",
    "    train, token2index, pad_token_index, unk_token_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features, val_labels, title_max_length, desc_max_lenght = preprocess_data(\n",
    "    val, token2index, pad_token_index, unk_token_index, title_max_length, desc_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels, title_max_length, desc_max_lenght = preprocess_data(\n",
    "    test, token2index, pad_token_index, unk_token_index, title_max_length, desc_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-leather",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "behind-leather",
    "outputId": "521e1a09-cb77-455f-e60d-1d59cac93f2b"
   },
   "outputs": [],
   "source": [
    "title_max_length, desc_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-columbus",
   "metadata": {
    "id": "cultural-columbus"
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_features[0], train_features[1], train_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_features[0], val_features[1], val_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_features[0], test_features[1], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-brick",
   "metadata": {
    "id": "starting-brick"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=gpu, \n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=gpu, \n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=gpu, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-interest",
   "metadata": {
    "id": "micro-interest"
   },
   "source": [
    "# Model Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-denver",
   "metadata": {
    "id": "illegal-denver"
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embeddings, embedding_dim, hidden_dim, num_classes, freeze_embeddings):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings, freeze=freeze_embeddings)\n",
    "        self.title_lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.desc_lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 4, num_classes) # Since BiLSTM + 2 inputs\n",
    "\n",
    "    def forward(self, title, description):\n",
    "        # (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        title_embedding = self.word_embeddings(title)\n",
    "        desc_embedding = self.word_embeddings(description)\n",
    "        # (batch_size, seq_len, embedding_dim) -> (batch_size, directions, hidden_dim) \n",
    "        _, (title_hidden, _) = self.title_lstm(title_embedding) \n",
    "        _, (desc_hidden, _) = self.desc_lstm(desc_embedding)\n",
    "        # (directions, batch_size, hidden_dim), (directions, batch_size, hidden_dim)\n",
    "        # -> (directions, batch_size, 2*hidden_dim)\n",
    "        out = torch.cat((title_hidden, desc_hidden), dim=2)\n",
    "        # (batch_size, directions, 2*hidden_dim) -> (batch_size, 2*hidden_dim*directions)\n",
    "        out = out.permute(1, 0, 2).flatten(start_dim=1)\n",
    "        # (batch_size, 2*hidden_dim*directions) -> (batch_size, num_classes)|\n",
    "        out = self.classifier(out)\n",
    "        # We use the CrossEntropyLoss so we aren't adding a softmax layer here\n",
    "        # Because in PyTorch CrossEntropyLoss combines a LogSoftmax with NLLLoss\n",
    "        # So we output raw logits\n",
    "        # Since we don't care about the confidences, we don't need a softmax during inference\n",
    "        # Since the highest value in a softmax will always be the highest value pre-softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-environment",
   "metadata": {
    "id": "enhanced-environment"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM(vectors, embedding_dim, nn_hidden_size, num_classes, freeze_embeddings)\n",
    "model.double() # Since our embeddings are 32-dimensional\n",
    "model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-ground",
   "metadata": {
    "id": "defined-ground"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-alarm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "matched-alarm",
    "outputId": "541cd067-23c1-469a-c5d0-b97018b08257"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # Move data to device\n",
    "        titles, descriptions, labels = batch\n",
    "        titles = titles.to(device)\n",
    "        descriptions = descriptions.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(titles, descriptions)\n",
    "        \n",
    "        # Calculate loss\n",
    "        batch_loss = loss_function(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update train loss\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    # Validation Loop\n",
    "    with torch.no_grad():\n",
    "        for iteration, batch in enumerate(tqdm(val_dataloader)):\n",
    "            # Move data to device\n",
    "            titles, descriptions, labels = batch\n",
    "            titles = titles.to(device)\n",
    "            descriptions = descriptions.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(titles, descriptions)\n",
    "\n",
    "            # Calculate loss\n",
    "            batch_loss = loss_function(predictions, labels)\n",
    "\n",
    "            # Update train loss\n",
    "            val_loss += batch_loss.item()\n",
    "    \n",
    "    # Compute the average losses for this epoch\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    \n",
    "    \n",
    "    # Print Metrics\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}/{epochs}, Train Loss = {train_loss}, \\\n",
    "        Validation Loss = {val_loss}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-annual",
   "metadata": {
    "id": "human-annual"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-repeat",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "early-repeat",
    "outputId": "4f0b8320-2dea-4e6a-db9b-d27788258ac4"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-netscape",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "choice-netscape",
    "outputId": "f7e5e7e1-a9e6-462b-9e5e-9a1e93eeb68f"
   },
   "outputs": [],
   "source": [
    "# Test Loop\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for iteration, batch in enumerate(tqdm(test_dataloader)):\n",
    "        # Move data to device\n",
    "        titles, descriptions, labels = batch\n",
    "        titles = titles.to(device)\n",
    "        descriptions = descriptions.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(titles, descriptions).detach().cpu().numpy().argmax(axis=1)\n",
    "        \n",
    "        y_true.extend(labels.detach().cpu().numpy())\n",
    "        y_pred.extend(predictions)\n",
    "y_true = indices_to_sentence(y_true, index2tag)\n",
    "y_pred = indices_to_sentence(y_pred, index2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-columbia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "divided-columbia",
    "outputId": "b6407432-f0a8-4e72-9c7d-7bdb394d6956"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-brazilian",
   "metadata": {
    "id": "quality-brazilian"
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-payday",
   "metadata": {
    "id": "drawn-payday"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oxI5svCjelpw",
   "metadata": {
    "id": "oxI5svCjelpw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
